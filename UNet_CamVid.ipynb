{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sk95068/Mahbub_MCSE/blob/main/UNet_CamVid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ps_41Ye4nJxi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfU5WpbunZNE",
        "outputId": "dcf6836f-18d8-4965-8a09-2ab3fb20a690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pxRUHoUvnquB"
      },
      "outputs": [],
      "source": [
        "def load_img(folder):\n",
        "  #an iter variable\n",
        "    images = []\n",
        "    labels = []\n",
        "    f = os.listdir(folder)\n",
        "    for filename in f: \n",
        "        img = cv2.imread(os.path.join(folder, filename)) \n",
        "        img = cv2.resize(img, (128, 128), interpolation = cv2.INTER_AREA)\n",
        "        lbl= cv2.imread(os.path.join(folder+'annot', filename))\n",
        "        lbl= cv2.resize(lbl, (128, 128), interpolation = cv2.INTER_AREA)\n",
        "        images.append(img)\n",
        "        labels.append(lbl)\n",
        "        #print(os.listdir(images))\n",
        "    return images, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kzEI20VMnznB"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform_img = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "transform_img_label = transforms.Compose([transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v0gH1Hy9Y_mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UKZODmjmodP9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#Advance Dataloaders\n",
        "\n",
        "class trainset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_train = None, root_train_label = None, transform_label = None):\n",
        "        self.train_img, self.train_labels = load_img(root_train)\n",
        "        self.transform = transform\n",
        "        self.transform_label = transform_label\n",
        "        #self.train_label_img = label_img_list(load_img(root_train_label))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.train_img)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.transform(self.train_img[index])\n",
        "        label = self.transform_label(self.train_labels[index])\n",
        "        \n",
        "        return img, label\n",
        "          \n",
        "class valset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_val = None, root_val_label = None, transform_label = None):\n",
        "        self.val_img, self.val_labels = load_img(root_val)\n",
        "        self.transform = transform\n",
        "        self.transform_label = transform_label\n",
        "        #self.test_label_img = label_img_list(load_img1(root_test_label))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.val_img)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.transform(self.val_img[index])\n",
        "        label = self.transform_label(self.val_labels[index])\n",
        "        \n",
        "        return img, label\n",
        "class testset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_test = None, root_test_label = None, transform_label = None):\n",
        "        self.val_img = load_img(root_test)\n",
        "        self.transform = transform\n",
        "        self.transform_label = transform_label\n",
        "        #self.test_label_img = label_img_list(load_img1(root_test_label))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.test_img)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.transform(self.test_img[index])\n",
        "        label = self.transform_label(self.test_label_img[index])\n",
        "        \n",
        "        return img, label        \n",
        "  \n",
        "\n",
        "traindataset = trainset(transform_img, '/content/gdrive/MyDrive/CamVid/CamVid/train', '/content/gdrive/MyDrive/CamVid/CamVid/trainannot', transform_img_label)\n",
        "\n",
        "testdataset = testset(transform_img , '/content/gdrive/MyDrive/CamVid/CamVid/test', '/content/gdrive/MyDrive/CamVid/CamVid/testannot', transform_img_label)\n",
        "\n",
        "valdataset = valset(transform_img, '/content/gdrive/MyDrive/CamVid/CamVid/val', '/content/gdrive/MyDrive/CamVid/CamVid/valannot', transform_img_label)\n",
        "\n",
        "       \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p4-7A1qBpsvO"
      },
      "outputs": [],
      "source": [
        "class u_net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 64, 3)\n",
        "      self.conv2 = nn.Conv2d(64, 128, 3)\n",
        "      self.conv3 = nn.Conv2d(128, 256, 3)\n",
        "      self.conv4 = nn.Conv2d(256, 512, 3)\n",
        "      self.conv5 = nn.Conv2d(512, 1024, 3)\n",
        "      self.conv6 = nn.Conv2d(1024, 512, 3)\n",
        "      self.conv7 = nn.Conv2d(512, 512, 3)\n",
        "      self.conv8 = nn.Conv2d(512, 256, 3)\n",
        "      self.conv9 = nn.Conv2d(256, 256, 3)\n",
        "      self.conv10 = nn.Conv2d(256, 128, 3)\n",
        "      self.conv11 = nn.Conv2d(128, 128, 3)\n",
        "      self.conv12 = nn.Conv2d(64, 64, 3)\n",
        "      self.b1 = nn.BatchNorm2d(64)\n",
        "      self.b2 = nn.BatchNorm2d(128)\n",
        "      self.b3 = nn.BatchNorm2d(256)\n",
        "      self.b4 = nn.BatchNorm2d(512)\n",
        "      self.b5 = nn.BatchNorm2d(1024)\n",
        "      self.convT1 = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
        "      self.convT2 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
        "      self.convT3 = nn.ConvTranspose2d(256, 256, 2, 2)\n",
        "      self.convT4 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
        "      self.convT5 = nn.ConvTranspose2d(64, 12, 2, 2)\n",
        "      self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.b1(self.conv1(x)))\n",
        "      x = F.relu(self.b1(self.conv12(x)))\n",
        "      x = F.relu(self.b2(self.conv2(x)))\n",
        "      x = self.pool1(x)\n",
        "      x = F.relu(self.b2(self.conv11(x)))\n",
        "      x = F.relu(self.b3(self.conv3(x)))\n",
        "      x1 = x\n",
        "      x1 = x1[:, :, int((58 - 24)/2) : int((58 + 24)/2), int((58 - 24)/2) : int((58 + 24)/2)];\n",
        "      x = self.pool1(x)\n",
        "      x = F.relu(self.b3(self.conv9(x)))\n",
        "      x = F.relu(self.b4(self.conv4(x)))\n",
        "      x2 = x\n",
        "      x2 = x2[:, :, int((25 - 16)/2) : int((25 + 16)/2), int((25 - 16)/2) : int((25 + 16)/2)];\n",
        "      x = self.pool1(x)\n",
        "      x = F.relu(self.b4(self.conv7(x)))\n",
        "      x = F.relu(self.b5(self.conv5(x)))\n",
        "      x = self.b4(self.convT1(x))\n",
        "      x = torch.cat((x2, x), dim = 1)\n",
        "      x = F.relu(self.b4(self.conv6(x)))\n",
        "      x = F.relu(self.b4(self.conv7(x)))\n",
        "      x = self.b3(self.convT2(x))\n",
        "      x = torch.cat((x1, x), dim = 1)\n",
        "      x = F.relu(self.b3(self.conv8(x)))\n",
        "      x = F.relu(self.b3(self.conv9(x)))\n",
        "      x = self.b3(self.convT3(x))\n",
        "      x = F.relu(self.b2(self.conv10(x)))\n",
        "      x = F.relu(self.b2(self.conv11(x)))\n",
        "      x = F.relu(self.b2(self.conv11(x)))\n",
        "      x = F.relu(self.b2(self.conv11(x)))\n",
        "      x = self.b1(self.convT4(x))\n",
        "      x = self.convT5(x)\n",
        "      del x1\n",
        "      del x2\n",
        "      return x\n",
        "net = u_net()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sLBF6CWOL1V",
        "outputId": "27ed1fd5-f3a9-4fba-f348-48bb5c7c7f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f2019c89950>\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "train_loader = data.DataLoader(traindataset, batch_size = 4, shuffle=True)\n",
        "val_loader =data.DataLoader(valdataset, batch_size=4,shuffle=False)\n",
        "\n",
        "print(train_loader)\n",
        "\n",
        "for images,label in train_loader:\n",
        "  #print(images)\n",
        "  #print(type(images))\n",
        "  x = torch.unique(label)*256\n",
        "  x = torch.round(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilmrV5eDp167",
        "outputId": "e37c996e-63c6-4b1b-b1ae-355dea204df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u_net(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv5): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv6): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv8): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv10): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (b1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (b2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (b3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (b4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (b5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (convT1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (convT2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (convT3): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (convT4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (convT5): ConvTranspose2d(64, 12, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "#UNET = net\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JNvPBf1ip77L"
      },
      "outputs": [],
      "source": [
        "loss1 = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.0001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zmdhpqRbR6p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vW2gncXXfvjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c5f5ff-27c8-4f57-ba52-9df18a29b764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Training\n",
            "model saved\n",
            "for epoch 0 ,training loss 43.609149158000946 validation loss tensor(2.1306, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 48 % IOU is tensor(0.8196)\n",
            "model saved\n",
            "for epoch 1 ,training loss 43.30864292383194 validation loss tensor(2.0965, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 52 % IOU is tensor(0.7993)\n",
            "for epoch 2 ,training loss 43.14097434282303 validation loss tensor(2.1192, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 50 % IOU is tensor(0.8426)\n",
            "model saved\n",
            "for epoch 3 ,training loss 42.975870221853256 validation loss tensor(2.0382, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.8382)\n",
            "for epoch 4 ,training loss 42.75690206885338 validation loss tensor(2.0710, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 55 % IOU is tensor(0.8145)\n",
            "for epoch 5 ,training loss 42.65739268064499 validation loss tensor(2.0756, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.7974)\n",
            "for epoch 6 ,training loss 42.43948698043823 validation loss tensor(2.1354, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 48 % IOU is tensor(0.8098)\n",
            "model saved\n",
            "for epoch 7 ,training loss 42.43406191468239 validation loss tensor(2.0206, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 60 % IOU is tensor(0.8564)\n",
            "for epoch 8 ,training loss 42.34449681639671 validation loss tensor(2.0975, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 51 % IOU is tensor(0.7931)\n",
            "model saved\n",
            "for epoch 9 ,training loss 42.176859974861145 validation loss tensor(2.0183, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.8741)\n",
            "for epoch 10 ,training loss 41.97207486629486 validation loss tensor(2.0811, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 53 % IOU is tensor(0.8483)\n",
            "for epoch 11 ,training loss 41.877640545368195 validation loss tensor(2.0936, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 52 % IOU is tensor(0.8493)\n",
            "for epoch 12 ,training loss 41.763568848371506 validation loss tensor(2.0899, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 52 % IOU is tensor(0.8337)\n",
            "for epoch 13 ,training loss 41.677796095609665 validation loss tensor(2.0778, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.8077)\n",
            "for epoch 14 ,training loss 41.618199467659 validation loss tensor(2.1333, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 48 % IOU is tensor(0.8327)\n",
            "for epoch 15 ,training loss 41.54388138651848 validation loss tensor(2.0411, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8695)\n",
            "for epoch 16 ,training loss 41.499905467033386 validation loss tensor(2.0993, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 51 % IOU is tensor(0.8299)\n",
            "for epoch 17 ,training loss 41.38005992770195 validation loss tensor(2.1028, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 51 % IOU is tensor(0.8368)\n",
            "for epoch 18 ,training loss 41.34001350402832 validation loss tensor(2.0873, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 52 % IOU is tensor(0.8428)\n",
            "for epoch 19 ,training loss 41.28234004974365 validation loss tensor(2.0790, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 53 % IOU is tensor(0.8357)\n",
            "for epoch 20 ,training loss 41.249881863594055 validation loss tensor(2.0439, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8740)\n",
            "for epoch 21 ,training loss 41.194599241018295 validation loss tensor(2.1117, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 50 % IOU is tensor(0.8479)\n",
            "for epoch 22 ,training loss 41.14985963702202 validation loss tensor(2.1289, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 48 % IOU is tensor(0.8162)\n",
            "for epoch 23 ,training loss 41.136093348264694 validation loss tensor(2.0873, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 53 % IOU is tensor(0.8284)\n",
            "for epoch 24 ,training loss 41.04672181606293 validation loss tensor(2.0948, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 52 % IOU is tensor(0.8323)\n",
            "for epoch 25 ,training loss 41.03150263428688 validation loss tensor(2.0402, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8687)\n",
            "for epoch 26 ,training loss 41.00134778022766 validation loss tensor(2.1052, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 51 % IOU is tensor(0.8419)\n",
            "for epoch 27 ,training loss 40.97684898972511 validation loss tensor(2.0655, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 55 % IOU is tensor(0.8372)\n",
            "model saved\n",
            "for epoch 28 ,training loss 40.94855457544327 validation loss tensor(2.0049, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 61 % IOU is tensor(0.9002)\n",
            "for epoch 29 ,training loss 40.92817670106888 validation loss tensor(2.0682, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.8324)\n",
            "model saved\n",
            "for epoch 30 ,training loss 40.904540836811066 validation loss tensor(1.9971, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 62 % IOU is tensor(0.9119)\n",
            "for epoch 31 ,training loss 40.88775855302811 validation loss tensor(2.1199, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 49 % IOU is tensor(0.8299)\n",
            "for epoch 32 ,training loss 40.855252265930176 validation loss tensor(2.0917, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 52 % IOU is tensor(0.8384)\n",
            "for epoch 33 ,training loss 40.8055704832077 validation loss tensor(2.0794, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 53 % IOU is tensor(0.8630)\n",
            "for epoch 34 ,training loss 40.741218626499176 validation loss tensor(2.0635, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 55 % IOU is tensor(0.8546)\n",
            "for epoch 35 ,training loss 40.7343510389328 validation loss tensor(2.1364, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 47 % IOU is tensor(0.8222)\n",
            "for epoch 36 ,training loss 40.78591328859329 validation loss tensor(2.1133, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 50 % IOU is tensor(0.8534)\n",
            "for epoch 37 ,training loss 40.921441465616226 validation loss tensor(2.1187, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 49 % IOU is tensor(0.8881)\n",
            "model saved\n",
            "for epoch 38 ,training loss 40.90934103727341 validation loss tensor(1.9845, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 63 % IOU is tensor(0.9547)\n",
            "for epoch 39 ,training loss 40.86447072029114 validation loss tensor(2.0674, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 55 % IOU is tensor(0.8479)\n",
            "for epoch 40 ,training loss 40.76103100180626 validation loss tensor(2.0193, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.9192)\n",
            "for epoch 41 ,training loss 40.571128249168396 validation loss tensor(2.0335, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.9064)\n",
            "for epoch 42 ,training loss 40.46054199337959 validation loss tensor(2.0249, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.9267)\n",
            "for epoch 43 ,training loss 40.408568024635315 validation loss tensor(2.0724, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.8625)\n",
            "for epoch 44 ,training loss 40.362958163022995 validation loss tensor(2.0541, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8885)\n",
            "for epoch 45 ,training loss 40.334863394498825 validation loss tensor(2.0289, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.9004)\n",
            "for epoch 46 ,training loss 40.320522487163544 validation loss tensor(2.0010, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 61 % IOU is tensor(0.9212)\n",
            "for epoch 47 ,training loss 40.30217117071152 validation loss tensor(2.0453, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8987)\n",
            "for epoch 48 ,training loss 40.29732006788254 validation loss tensor(2.0144, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 60 % IOU is tensor(0.9190)\n",
            "for epoch 49 ,training loss 40.2919999063015 validation loss tensor(2.0075, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 61 % IOU is tensor(0.9272)\n",
            "for epoch 50 ,training loss 40.272353172302246 validation loss tensor(2.0644, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 55 % IOU is tensor(0.8726)\n",
            "for epoch 51 ,training loss 40.252056539058685 validation loss tensor(2.0252, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.9078)\n",
            "for epoch 52 ,training loss 40.243119448423386 validation loss tensor(2.0180, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 60 % IOU is tensor(0.9011)\n",
            "for epoch 53 ,training loss 40.238166093826294 validation loss tensor(2.0024, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 61 % IOU is tensor(0.9183)\n",
            "for epoch 54 ,training loss 40.22869265079498 validation loss tensor(2.0045, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 61 % IOU is tensor(0.9256)\n",
            "for epoch 55 ,training loss 40.20986157655716 validation loss tensor(2.0559, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8642)\n",
            "for epoch 56 ,training loss 40.18315815925598 validation loss tensor(2.0182, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.9152)\n",
            "for epoch 57 ,training loss 40.16884791851044 validation loss tensor(2.0297, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.9159)\n",
            "for epoch 58 ,training loss 40.131485641002655 validation loss tensor(2.0275, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.8994)\n",
            "for epoch 59 ,training loss 40.09434658288956 validation loss tensor(2.0758, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.8741)\n",
            "for epoch 60 ,training loss 40.06987136602402 validation loss tensor(2.0730, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.8658)\n",
            "for epoch 61 ,training loss 40.088836669921875 validation loss tensor(2.0721, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.8997)\n",
            "for epoch 62 ,training loss 40.11092311143875 validation loss tensor(2.0514, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.9175)\n",
            "for epoch 63 ,training loss 40.04996341466904 validation loss tensor(2.0656, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 55 % IOU is tensor(0.8941)\n",
            "for epoch 64 ,training loss 40.02503392100334 validation loss tensor(2.0309, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.9119)\n",
            "for epoch 65 ,training loss 39.947001963853836 validation loss tensor(2.0241, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.9157)\n",
            "for epoch 66 ,training loss 39.9010873734951 validation loss tensor(2.0565, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.9041)\n",
            "for epoch 67 ,training loss 39.86006352305412 validation loss tensor(2.0157, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 60 % IOU is tensor(0.9121)\n",
            "for epoch 68 ,training loss 39.88246867060661 validation loss tensor(2.0766, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 53 % IOU is tensor(0.8255)\n",
            "for epoch 69 ,training loss 39.823453068733215 validation loss tensor(2.0559, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8779)\n",
            "for epoch 70 ,training loss 39.7717881500721 validation loss tensor(1.9898, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 62 % IOU is tensor(0.9228)\n",
            "for epoch 71 ,training loss 39.738492518663406 validation loss tensor(2.0510, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8618)\n",
            "for epoch 72 ,training loss 39.71696799993515 validation loss tensor(2.0399, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.9082)\n",
            "for epoch 73 ,training loss 39.70238360762596 validation loss tensor(2.0406, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8875)\n",
            "for epoch 74 ,training loss 39.69263803958893 validation loss tensor(2.0303, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.9047)\n",
            "for epoch 75 ,training loss 39.69011950492859 validation loss tensor(2.0417, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8789)\n",
            "for epoch 76 ,training loss 39.68098047375679 validation loss tensor(2.0432, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8713)\n",
            "for epoch 77 ,training loss 39.67249232530594 validation loss tensor(2.0451, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8889)\n",
            "for epoch 78 ,training loss 39.659187376499176 validation loss tensor(2.0251, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.9210)\n",
            "for epoch 79 ,training loss 39.65437290072441 validation loss tensor(2.0296, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.8976)\n",
            "for epoch 80 ,training loss 39.650423020124435 validation loss tensor(2.0495, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8804)\n",
            "for epoch 81 ,training loss 39.64059308171272 validation loss tensor(2.0767, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.8697)\n",
            "for epoch 82 ,training loss 39.63246989250183 validation loss tensor(2.0476, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8714)\n",
            "for epoch 83 ,training loss 39.62546989321709 validation loss tensor(2.0326, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.8980)\n",
            "for epoch 84 ,training loss 39.61937794089317 validation loss tensor(2.0234, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.9062)\n",
            "for epoch 85 ,training loss 39.61993435025215 validation loss tensor(2.0373, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.8694)\n",
            "for epoch 86 ,training loss 39.61998489499092 validation loss tensor(2.0667, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 55 % IOU is tensor(0.8620)\n",
            "for epoch 87 ,training loss 39.61439719796181 validation loss tensor(2.0211, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 59 % IOU is tensor(0.8999)\n",
            "for epoch 88 ,training loss 39.607890129089355 validation loss tensor(2.0526, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8747)\n",
            "for epoch 89 ,training loss 39.6054171025753 validation loss tensor(2.0148, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 60 % IOU is tensor(0.9041)\n",
            "for epoch 90 ,training loss 39.59397283196449 validation loss tensor(2.0514, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8886)\n",
            "for epoch 91 ,training loss 39.58335244655609 validation loss tensor(2.0365, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.8835)\n",
            "for epoch 92 ,training loss 39.586495757102966 validation loss tensor(2.0570, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 55 % IOU is tensor(0.8683)\n",
            "for epoch 93 ,training loss 39.58667799830437 validation loss tensor(2.0349, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 58 % IOU is tensor(0.8840)\n",
            "for epoch 94 ,training loss 39.593011885881424 validation loss tensor(2.0741, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 54 % IOU is tensor(0.8636)\n",
            "for epoch 95 ,training loss 39.58389836549759 validation loss tensor(2.0172, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 60 % IOU is tensor(0.9218)\n",
            "for epoch 96 ,training loss 39.570033848285675 validation loss tensor(2.0538, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8565)\n",
            "for epoch 97 ,training loss 39.55649074912071 validation loss tensor(2.0525, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8835)\n",
            "for epoch 98 ,training loss 39.5397162437439 validation loss tensor(2.0537, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 56 % IOU is tensor(0.8665)\n",
            "for epoch 99 ,training loss 39.53096294403076 validation loss tensor(2.0394, device='cuda:0', grad_fn=<NllLoss2DBackward0>) accuracy : 57 % IOU is tensor(0.8854)\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "PATH = './camvid.pth'\n",
        "val_loss_min=999999\n",
        "net_min=net.state_dict()\n",
        "otp_min=optimizer.state_dict()\n",
        "loss_set=[]\n",
        "#net= net.cuda()\n",
        "print('Start Training')\n",
        "for epoch in range(100):  \n",
        "    train_loss = 0.0\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader,0):\n",
        "      # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        #inputs=inputs.cuda()\n",
        "        #labels=labels.cuda()\n",
        "        x = labels*256\n",
        "        x = torch.round(x)\n",
        "        labels = x\n",
        "        \n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        outputs = nn.functional.softmax(outputs, dim=1)\n",
        "        labels = labels[:,0,:,:]\n",
        "        labels=labels.squeeze(1)\n",
        "        labels = labels.long()\n",
        "        loss = loss1(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "        running_loss += loss.item()\n",
        "        optimizer.step()\n",
        "       \n",
        "    train_loss = running_loss/4.0\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        correct_val=0\n",
        "        total_val =0\n",
        "        val_loss =0.0\n",
        "        inputs, labels = data\n",
        "        #inputs=inputs.cuda()\n",
        "        #labels=labels.cuda()\n",
        "        x = labels*256\n",
        "        x = torch.round(x)\n",
        "        labels = x\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        outputs = nn.functional.softmax(outputs, dim=1)\n",
        "        labels = labels[:,0,:,:]\n",
        "        labels=labels.squeeze(1)\n",
        "        labels = labels.long()\n",
        "        \n",
        "        val_loss = loss1(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, dim=1) \n",
        "        total_val+=labels.size(0)\n",
        "        correct_val += (predicted == labels).sum().item()\n",
        "        total_val += labels.nelement()\n",
        "        #IOU_score\n",
        "        intersection = np.logical_and(labels.cpu(), predicted.cpu())\n",
        "        union = np.logical_or(labels.cpu(),predicted.cpu())\n",
        "        iou_score = torch.sum(intersection)/torch.sum(union)\n",
        "        \n",
        "        \n",
        "    if(val_loss<val_loss_min):\n",
        "\n",
        "        net_min=net.state_dict()\n",
        "        otp_min=optimizer.state_dict()\n",
        "        \n",
        "        torch.save(net.state_dict(), PATH)\n",
        "        print(\"model saved\")\n",
        "        val_loss_min= val_loss\n",
        "    val_accuracy = ((100*correct_val)/total_val)\n",
        "    loss_set.append([train_loss,val_loss])\n",
        "\n",
        "    print(\"for epoch\",epoch,\",training loss\",train_loss,\"validation loss\",val_loss  , \"accuracy : %d %%\" %(val_accuracy), \"IOU is %s\" %iou_score)\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xhHyK-IZU3He"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "test_loader = torch.utils.data.DataLoader(testdataset, batch_size = 4, shuffle=False)\n",
        "for images,label in train_loader:\n",
        "  x = torch.unique(label)*256\n",
        "  x = torch.round(x)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_OCwU7Ti5Oz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f102fc-03e1-439f-98c2-1e0f49282e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 300 val images: 48 %\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "# look at how the network performs on the whole dataset.\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in val_loader:\n",
        "        inputs, labels = data\n",
        "        x = labels*256\n",
        "        x = torch.round(x)\n",
        "        labels = x\n",
        "        #inputs=inputs.cuda()\n",
        "        #labels=labels.cuda()\n",
        "        inputs, labels = inputs.to(device), labels.to(device) \n",
        "        outputs = net(inputs)\n",
        "        outputs = nn.functional.softmax(outputs, dim=1)\n",
        "        labels = labels[:,0,:,:]\n",
        "        labels=labels.squeeze(1)\n",
        "        labels = labels.long()\n",
        "        val_loss = loss1(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, dim=1)        \n",
        "        total_val+=labels.size(0)\n",
        "        correct_val += (predicted == labels).sum().item()\n",
        "        total_val += labels.nelement()        \n",
        "        # calculate outputs by running images through the network        \n",
        "    val_accuracy = ((100*correct_val)/total_val)\n",
        "print('Accuracy of the network on the 300 val images: %d %%' % (val_accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "iUwVUSS7NEOj",
        "outputId": "26fe5bb8-30c9-4c02-961f-877fd8f06a3c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_loss_line=[]\n",
        "val_loss_line=[]\n",
        "x=[]\n",
        "for i in range(len(loss_set)):\n",
        "  train_loss_line.append(loss_set[i][0])\n",
        "  val_loss_line.append(loss_set[i][1])\n",
        "  x.append(i)\n",
        "plt.plot(x, train_loss_line, label = \"training loss\")\n",
        "plt.plot(x, val_loss_line , label = \"validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "with torch.no_grad():\n",
        "    for data in val_loader:\n",
        "        inputs, labels = data\n",
        "        x = labels*256\n",
        "        x = torch.round(x)\n",
        "        labels = x\n",
        "\n",
        "        inter =np.logical_and(labels.cpu(), predicted.cpu())\n",
        "        union =np.logical_or(labels.cpu(),predicted.cpu())\n",
        "        iou_score= torch.sum(inter)/torch.sum(union)\n",
        "print('IOU is %s' %iou_score)\n",
        "\n",
        "\n",
        "\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLcBMmjs3-dw",
        "outputId": "30fab51d-418c-412c-a556-65ff2f10cc32"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IOU is tensor(0.8854)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "UNet_CamVid.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqOGiSX7E6FzHc4L9hEepA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}